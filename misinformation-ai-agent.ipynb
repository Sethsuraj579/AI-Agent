{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8022663,"sourceType":"datasetVersion","datasetId":4727630}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Kaggle Environment Setup","metadata":{}},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\n\ntry:\n    GOOGLE_API_KEY = UserSecretsClient().get_secret(\"API\")\n    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n    os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"FALSE\"\n    print(\"âœ… Gemini API key setup complete.\")\nexcept Exception as e:\n    print(f\"ðŸ”‘ Authentication Error: Please make sure you have added 'GOOGLE_API_KEY' to your Kaggle secrets. Details: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport requests\nimport subprocess\nimport time\nimport uuid\n\nfrom google.adk.agents import LlmAgent\nfrom google.adk.agents.remote_a2a_agent import (\n    RemoteA2aAgent,\n    AGENT_CARD_WELL_KNOWN_PATH,\n)\n\nfrom google.adk.a2a.utils.agent_to_a2a import to_a2a\nfrom google.adk.models.google_llm import Gemini\nfrom google.adk.runners import Runner\nfrom google.adk.sessions import InMemorySessionService\nfrom google.genai import types\n\n# Hide additional warnings in the notebook\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"âœ… ADK components imported successfully.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"retry_config = types.HttpRetryOptions(\n    attempts=5,  # Maximum retry attempts\n    exp_base=7,  # Delay multiplier\n    initial_delay=1,\n    http_status_codes=[429, 500, 503, 504],  # Retry on these HTTP errors\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Making Model","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(\"/kaggle/input/liar-dataset/train.tsv\", sep='\\t', header=None)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns = [\n    \"label\", \"statement\", \"subject\", \"speake\", \"job_title\",\n    \"state_info\", \"party_affiliation\", \"barely_true_counts\",\n    \"false_counts\", \"half_true_counts\", \"mostly_true_counts\",\n    \"pants_on_fire_counts\", \"context\",\"data\"\n]\n\ndf.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"texts = df['statement'].tolist()\nlabels = df['label'].map({'true':1, 'false':0}).tolist()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n\n# Tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nencodings = tokenizer(texts, truncation=True, padding=True)\n\n# Dataset class\nclass ClaimDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n    def __len__(self):\n        return len(self.labels)\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\ndataset = ClaimDataset(encodings, labels)\n\n#Model\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n\n# Training\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=2,\n    per_device_train_batch_size=16,\n    save_strategy=\"epoch\"\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset\n)\n\ntrainer.train()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install transformers sentence-transformers datasets evaluate accelerate --quiet\nfrom typing import List, Dict, Any\nfrom dataclasses import dataclass\n\nfrom transformers import (\n    AutoTokenizer, AutoModelForSequenceClassification,\n    Trainer, TrainingArguments\n)\nfrom sentence_transformers import SentenceTransformer, util\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Global config\nCONFIG = {\n    \"retriever_model\": \"gemini-2.5-flash-lite\",\n    \"verifier_model\": \"roberta-base\",     # replace with your fine-tuned checkpoint later\n    \"top_k_evidence\": 5,\n    \"memory_dir\": \"./radar_memory\",\n    \"log_file\": \"./radar_logs.jsonl\"\n}\n\nos.makedirs(CONFIG[\"memory_dir\"], exist_ok=True)\n\n# Helpers: logging + memory\ndef log_event(agent: str, stage: str, payload: Dict[str, Any]):\n    event = {\n        \"ts\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n        \"agent\": agent,\n        \"stage\": stage,\n        \"payload\": payload\n    }\n    with open(CONFIG[\"log_file\"], \"a\") as f:\n        f.write(json.dumps(event) + \"\\n\")\n\ndef memory_write(key: str, value: Dict[str, Any]):\n    path = os.path.join(CONFIG[\"memory_dir\"], f\"{key}.json\")\n    with open(path, \"w\") as f:\n        json.dump(value, f)\n\ndef memory_read(key: str):\n    path = os.path.join(CONFIG[\"memory_dir\"], f\"{key}.json\")\n    return json.load(open(path)) if os.path.exists(path) else None\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_claim(text: str) -> str:\n    \"\"\"\n    Minimal baseline: trims and returns the main sentence.\n    Extend with NER, dependency parsing, or heuristic rules to isolate factual propositions.\n    \"\"\"\n    claim = text.strip()\n    log_event(\"ClaimExtractor\", \"extracted\", {\"input\": text, \"claim\": claim})\n    return claim\n    \n\ncorpus_df = pd.DataFrame({\n    \"source\": [\"Wikipedia\", \"Snopes\", \"WHO\"],\n    \"snippet\": [\n        \"Vaccines undergo rigorous testing for safety and efficacy.\",\n        \"Claims about microchips in vaccines have been repeatedly debunked.\",\n        \"WHO states that approved COVID-19 vaccines are safe and effective.\"\n    ]\n})\n\nCORPUS = corpus_df[\"snippet\"].tolist()\n\nretriever = LlmAgent(\n    name=\"retriever\",  # required field\n    instruction=\"\\n\".join(CORPUS),  # must be a string, not DataFrame\n    model=Gemini(model=CONFIG[\"retriever_model\"],\n                tools=[extract_claim])  # use the correct field name\n)\nprint(\"âœ… Here is fact checker Agent created successfully!\")\nprint(\"   Model: gemini-2.5-flash-lite\")\nprint(\"   Tool: extract_claim()\")\nprint(\"   Ready to be exposed via A2A...\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test the agent\nfrom google.adk.runners import InMemoryRunner\ncurrent_fact = InMemoryRunner(agent=retriever)\n_ = await current_fact.run_debug(\n    \"Is it true, that COVID have make a sheid it self? How it make it?\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}